# Building of Computationally Effective Deep Learning Models using Attention-Guided Knowledge Distillation

Code used during research for the paper: **Building of Computationally Effective Deep Learning Models using Attention-Guided Knowledge Distillation**

Accepted in the **12th International Conference on Advanced Computing - ICoAC 2023** (http://www.icoac.mitindia.edu/).

**Status:** Published ([Link to paper](https://ieeexplore.ieee.org/document/10249491))

The repository holds:
1. Jupyter notebooks for knowledge distillation using all attention capture mechanisms, vanilla knowledge distillation, and the usual transfer learning method.
2. Weights of the best-performing model for each method.
3. Sample intermediate attention maps for each method.
4. Train Test split of the [PAD-UFES-20](https://www.sciencedirect.com/science/article/pii/S235234092031115X) dataset used for the experiments.
